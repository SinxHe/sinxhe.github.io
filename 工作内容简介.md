工作内容简介:
1. AI生图: 围绕大模型AIGC能力落地XEva写真产品, 从零开始独立负责技术侧整个生命周期(需求评审/技术调研/服务开发/管理后台全栈开发)
2. 小冰岛寻路模块: 独立负责寻路模块需求评审/架构设计/功能开发, 抽象Unity导出地图数据, 服务端构建地图模型实现NPC寻路避障
3. 小冰对话平台: 维护基础组件&服务, 围绕小冰对话需求, 开发短链, 短信, 压图, SR等微服务

自我评价: 

4年大模型应用开发经验，5年大数据开发经验, 能独立负责单个产品线/模块评审开发. 能快速学习新技术, 适应新环境.


模型的微调也好, 各种框架的搭建也好, 本身就是支持业务的发展, 现在AI的技术壁垒并没有那么大. 如果能够很好的将业务和底层打通, 减少中间信息流通的沉默成本, 更能事半功倍.

AGI（Artificial General Intelligence，人工通用智能）
AIGC（AI-Generated Content，人工智能生成内容）
UGC（User-Generated Content，用户生成内容）
RAG（Retrieval-Augmented Generation，检索增强生成）
SFT（Supervised Fine-Tuning，监督微调）
2. SFT（Supervised Fine-Tuning，监督微调）
定义：使用标注数据对预训练模型进行有监督的精细调整
训练流程：
预训练（Pretraining）：海量无标注数据学习语言模式
监督微调：标注数据（输入-输出对）调整模型行为
应用场景：
任务定制化（如客服对话模型）
安全对齐（过滤有害内容）

预训练模型（PT）
  ├─监督微调（SFT）
  │  ├─应用技术：Prompt Engineering
  │  └─对齐方法：RLHF/DPO
  │
  ├─高效微调：LoRA/QLoRA
  │
  └─增强架构
      ├─检索增强：RAG
      └─混合专家：MoE


术语	全称	核心概念
RLHF	Reinforcement Learning from Human Feedback	通过人类反馈进行强化学习对齐模型价值观
DPO	Direct Preference Optimization	直接偏好优化，替代RLHF的新对齐方法
LoRA	Low-Rank Adaptation	低秩适配，高效微调大模型的参数高效方法
PT	Pretraining	预训练阶段，模型学习通用语言表征